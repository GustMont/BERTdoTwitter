{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Aplicando BERT para Detecção de Bots do Twitter"
      ],
      "metadata": {
        "id": "ZPCRP3ihl_u1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aluno: Gustavo Monteiro"
      ],
      "metadata": {
        "id": "Pvwgvp51mHYb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciZZ52nqnAt-",
        "outputId": "5bf4d86c-a1ca-49c7-ae4e-356c5536c475"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow transformers pandas scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkGlAKFIeIbu",
        "outputId": "05589ee5-4260-4530-8fdc-dcb14fda90b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow transformers scikit-learn pandas gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download do Dataset"
      ],
      "metadata": {
        "id": "7kLk--oJj98q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O dataset foi obtido via Google Drive usando o módulo gdown. O arquivo foi salvo localmente como twitter_bot_detection.csv e carregado utilizando o pandas."
      ],
      "metadata": {
        "id": "G53zBQHNj_tK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "8RgQyEMaeKfB",
        "outputId": "5c516f4f-9339-42c9-d768-36ccdd5b25ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1et-4zd_KETINZmIzjTET3YNfru3LhDcB\n",
            "To: /content/twitter_bot_detection.csv\n",
            "100%|██████████| 7.46M/7.46M [00:00<00:00, 123MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'twitter_bot_detection.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import gdown\n",
        "url = \"https://drive.google.com/uc?id=1et-4zd_KETINZmIzjTET3YNfru3LhDcB\"\n",
        "\n",
        "output = \"twitter_bot_detection.csv\"\n",
        "gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cfbz6tdnePiV",
        "outputId": "c4275ba2-6025-4a47-9c98-3d74c6a8d063"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   User ID        Username                                              Tweet  \\\n",
            "0   132131           flong  Station activity person against natural majori...   \n",
            "1   289683  hinesstephanie  Authority research natural life material staff...   \n",
            "2   779715      roberttran  Manage whose quickly especially foot none to g...   \n",
            "3   696168          pmason  Just cover eight opportunity strong policy which.   \n",
            "4   704441          noah87                      Animal sign six data good or.   \n",
            "\n",
            "   Retweet Count  Mention Count  Follower Count  Verified  Bot Label  \\\n",
            "0             85              1            2353     False          1   \n",
            "1             55              5            9617      True          0   \n",
            "2              6              2            4363      True          0   \n",
            "3             54              5            2242      True          1   \n",
            "4             26              3            8438     False          1   \n",
            "\n",
            "       Location           Created At            Hashtags  \n",
            "0     Adkinston  2020-05-11 15:29:50                 NaN  \n",
            "1    Sanderston  2022-11-26 05:18:10           both live  \n",
            "2  Harrisonfurt  2022-08-08 03:16:54         phone ahead  \n",
            "3  Martinezberg  2021-08-14 22:27:05  ever quickly new I  \n",
            "4  Camachoville  2020-04-13 21:24:21     foreign mention  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"twitter_bot_detection.csv\")\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pré-processamento"
      ],
      "metadata": {
        "id": "hW9inMspkDu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MQtEJCAmkFFy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiq3CRfcmDD7",
        "outputId": "ccac2733-3934-4a75-af9a-b66511380e9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   User ID        Username                                              Tweet  \\\n",
            "0   132131           flong  Station activity person against natural majori...   \n",
            "1   289683  hinesstephanie  Authority research natural life material staff...   \n",
            "2   779715      roberttran  Manage whose quickly especially foot none to g...   \n",
            "3   696168          pmason  Just cover eight opportunity strong policy which.   \n",
            "4   704441          noah87                      Animal sign six data good or.   \n",
            "\n",
            "   Retweet Count  Mention Count  Follower Count  Verified  Bot Label  \\\n",
            "0             85              1            2353     False          1   \n",
            "1             55              5            9617      True          0   \n",
            "2              6              2            4363      True          0   \n",
            "3             54              5            2242      True          1   \n",
            "4             26              3            8438     False          1   \n",
            "\n",
            "       Location           Created At            Hashtags  \n",
            "0     Adkinston  2020-05-11 15:29:50                 NaN  \n",
            "1    Sanderston  2022-11-26 05:18:10           both live  \n",
            "2  Harrisonfurt  2022-08-08 03:16:54         phone ahead  \n",
            "3  Martinezberg  2021-08-14 22:27:05  ever quickly new I  \n",
            "4  Camachoville  2020-04-13 21:24:21     foreign mention  \n",
            "Index(['User ID', 'Username', 'Tweet', 'Retweet Count', 'Mention Count',\n",
            "       'Follower Count', 'Verified', 'Bot Label', 'Location', 'Created At',\n",
            "       'Hashtags'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "df = pd.read_csv(\"twitter_bot_detection.csv\")\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "df = df.dropna().drop_duplicates()\n",
        "\n",
        "print(df.columns)\n",
        "\n",
        "texts = df['Tweet'].values\n",
        "labels = df['Bot Label'].values\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, encoded_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def tokenize_texts(texts, tokenizer, max_len=128):\n",
        "    encodings = tokenizer(\n",
        "        list(texts),\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "    return encodings\n",
        "\n",
        "train_encodings = tokenize_texts(X_train, tokenizer)\n",
        "test_encodings = tokenize_texts(X_test, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remoção de valores nulos e duplicados.\n",
        "Codificação dos textos dos tweets usando o BERT Tokenizer, que prepara os dados de entrada para serem compatíveis com o modelo BERT.\n",
        "Divisão dos dados em treino e teste, com uma proporção de 80% para treino e 20% para teste, usando train_test_split."
      ],
      "metadata": {
        "id": "cde2lP2HkFJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementação do Modelo"
      ],
      "metadata": {
        "id": "wJlQF5T7ka3R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CfGoVKqjYko",
        "outputId": "dda3faad-6535-4b60-8da3-322948ec4c9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (lambda), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'tf_bert_model/bert/embeddings/word_embeddings/weight:0' shape=(30522, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/embeddings/token_type_embeddings/embeddings:0' shape=(2, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/embeddings/position_embeddings/embeddings:0' shape=(512, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._0/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._0/attention/self/query/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._0/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._0/attention/self/key/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._0/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._0/attention/self/value/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._0/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._0/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._0/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._0/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._0/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._0/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._0/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._0/output/dense/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._0/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._0/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._1/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._1/attention/self/query/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._1/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._1/attention/self/key/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._1/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._1/attention/self/value/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._1/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._1/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._1/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._1/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._1/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._1/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._1/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._1/output/dense/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._1/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._1/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._2/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._2/attention/self/query/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._2/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._2/attention/self/key/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._2/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._2/attention/self/value/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._2/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._2/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._2/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._2/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._2/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._2/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._2/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._2/output/dense/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._2/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._2/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._3/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._3/attention/self/query/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._3/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._3/attention/self/key/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._3/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._3/attention/self/value/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._3/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._3/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._3/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._3/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._3/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._3/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._3/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._3/output/dense/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._3/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._3/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._4/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._4/attention/self/query/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._4/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._4/attention/self/key/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._4/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._4/attention/self/value/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._4/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._4/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._4/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._4/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._4/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._4/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._4/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._4/output/dense/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._4/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._4/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._5/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._5/attention/self/query/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._5/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._5/attention/self/key/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._5/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._5/attention/self/value/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._5/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._5/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._5/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._5/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._5/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._5/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._5/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._5/output/dense/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._5/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._5/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._6/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._6/attention/self/query/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._6/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._6/attention/self/key/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._6/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._6/attention/self/value/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._6/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._6/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._6/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._6/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._6/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._6/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._6/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._6/output/dense/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._6/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._6/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._7/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._7/attention/self/query/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._7/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._7/attention/self/key/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._7/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._7/attention/self/value/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._7/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._7/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._7/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._7/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._7/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._7/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._7/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._7/output/dense/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._7/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._7/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._8/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._8/attention/self/query/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._8/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._8/attention/self/key/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._8/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._8/attention/self/value/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._8/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._8/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._8/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._8/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._8/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._8/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._8/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._8/output/dense/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._8/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._8/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._9/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._9/attention/self/query/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._9/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._9/attention/self/key/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._9/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._9/attention/self/value/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._9/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._9/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._9/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._9/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._9/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._9/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._9/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._9/output/dense/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._9/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._9/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._10/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._10/attention/self/query/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._10/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._10/attention/self/key/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._10/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._10/attention/self/value/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._10/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._10/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._10/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._10/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._10/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._10/output/dense/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._10/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._10/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._11/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._11/attention/self/query/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._11/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._11/attention/self/key/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._11/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._11/attention/self/value/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._11/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._11/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._11/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._11/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._11/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._11/output/dense/bias:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/pooler/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
            "  <tf.Variable 'tf_bert_model/bert/pooler/dense/bias:0' shape=(768,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)      [(None, 128)]                0         []                            \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer  [(None, 128)]                0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " lambda (Lambda)             (None, 128, 768)             0         ['input_ids[0][0]',           \n",
            "                                                                     'attention_mask[0][0]']      \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (  (None, 768)                  0         ['lambda[0][0]']              \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 1)                    769       ['tf.__operators__.getitem[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 769 (3.00 KB)\n",
            "Trainable params: 769 (3.00 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFBertModel\n",
        "\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"input_ids\")\n",
        "attention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"attention_mask\")\n",
        "\n",
        "def bert_lambda(inputs):\n",
        "    return bert_model(input_ids=inputs[0], attention_mask=inputs[1]).last_hidden_state\n",
        "\n",
        "bert_outputs = tf.keras.layers.Lambda(bert_lambda, output_shape=(128, 768))([input_ids, attention_mask])\n",
        "\n",
        "cls_token = bert_outputs[:, 0, :]\n",
        "\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid')(cls_token)\n",
        "\n",
        "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A saída do modelo foi conectada a uma camada densa com ativação sigmoid, que retorna a probabilidade de uma amostra ser bot ou humano."
      ],
      "metadata": {
        "id": "cmDO5wCBkj2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinamento do Modelo"
      ],
      "metadata": {
        "id": "yie-BCQekkyd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "id": "c6s1dFthvUDO",
        "outputId": "f607c7e1-d3e4-4045-f432-6a6794ac7a5b"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ attention_mask            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">769</span> │ get_item[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_ids (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ attention_mask            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lambda (\u001b[38;5;33mLambda\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ input_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ attention_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item (\u001b[38;5;33mGetItem\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m769\u001b[0m │ get_item[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">769</span> (3.00 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m769\u001b[0m (3.00 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">769</span> (3.00 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m769\u001b[0m (3.00 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m417/417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30s/step - accuracy: 0.4919 - loss: 0.7929 "
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-15c15c5db684>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Treinar o modelo com early stopping e batch_size ajustado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_encodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_encodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Entradas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    341\u001b[0m                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                     )\n\u001b[0;32m--> 343\u001b[0;31m                 val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m    344\u001b[0m                     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m                 \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m                 \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1553\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFBertModel\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"input_ids\")\n",
        "attention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"attention_mask\")\n",
        "\n",
        "def bert_lambda(inputs):\n",
        "    return bert_model(input_ids=inputs[0], attention_mask=inputs[1]).last_hidden_state\n",
        "\n",
        "bert_outputs = tf.keras.layers.Lambda(bert_lambda, output_shape=(128, 768))([input_ids, attention_mask])\n",
        "\n",
        "cls_token = bert_outputs[:, 0, :]\n",
        "\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid')(cls_token)\n",
        "\n",
        "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "history = model.fit(\n",
        "    x=[train_encodings['input_ids'], train_encodings['attention_mask']],\n",
        "    y=y_train,\n",
        "    validation_split=0.2,\n",
        "    batch_size=batch_size,\n",
        "    epochs=2,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Otimizador Adam com uma taxa de aprendizado de 2e-5 e função de perda binary_crossentropy\n",
        "\n",
        "EarlyStopping para interromper o treinamento caso não houvesse melhorias no erro de validação\n",
        "\n",
        "batch_size para otimizar o tempo de treinamento, visto que o modelo estava demorando em cada época devido à complexidade e tamanho dos dados"
      ],
      "metadata": {
        "id": "QatuQGPEk8hQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
        "\n",
        "y_pred = model.predict([test_encodings['input_ids'], test_encodings['attention_mask']])\n",
        "y_pred_labels = (y_pred > 0.5).astype(int)\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred_labels))\n",
        "print(classification_report(y_test, y_pred_labels))\n",
        "print(f\"AUC-ROC: {roc_auc_score(y_test, y_pred)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgbnTOLDEpxW",
        "outputId": "0cf91645-3a03-4068-e1ba-6398b940f590"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3889s\u001b[0m 15s/step\n",
            "[[2786 1419]\n",
            " [2712 1415]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.66      0.57      4205\n",
            "           1       0.50      0.34      0.41      4127\n",
            "\n",
            "    accuracy                           0.50      8332\n",
            "   macro avg       0.50      0.50      0.49      8332\n",
            "weighted avg       0.50      0.50      0.49      8332\n",
            "\n",
            "AUC-ROC: 0.5067604738609781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objetivo\n"
      ],
      "metadata": {
        "id": "OL0ZY84Ojpxg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O objetivo deste projeto foi treinar uma rede neural BERT no Keras para detectar bots no Twitter com o dataset \"Twitter-Bot Detection\", e o modelo pré-treinado BERT (Bidirectional Encoder Representations from Transformers) para realizar a tarefa de classificação binária (humano ou bot)."
      ],
      "metadata": {
        "id": "e5Oem6n-jtyv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Desafios Enfrentados"
      ],
      "metadata": {
        "id": "9M9AKbWrj3bU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Erro de Formato nos Inputs: Durante o processo de implementação, encontramos um erro de formato no modelo relacionado à compatibilidade dos tensores de entrada com o esperado pelo modelo BERT"
      ],
      "metadata": {
        "id": "KcgROnPTlN2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tempo de Treinamento Prolongado: O treinamento inicial apresentou uma lentidão significativa, com algumas épocas durando horas. Para mitigar esse problema, o ajuste do batch_size para 64, além de adicionar a técnica de EarlyStopping para evitar o desperdício de tempo em épocas sem melhorias"
      ],
      "metadata": {
        "id": "ybje0czTlSVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lições Aprendidas\n"
      ],
      "metadata": {
        "id": "MmPkqWWblaUK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O sucesso do modelo BERT depende fortemente da qualidade e do formato dos dados de entrada. Tokenizar corretamente os textos e garantir que os tensores de entrada estejam no formato correto.\n",
        "\n",
        "O treinamento de modelos complexos como o BERT pode ser muito demorado. Aprendemos que ajustar o batch_size e aplicar técnicas de interrupção antecipada, como o EarlyStopping para otimizar o uso de recursos e o tempo."
      ],
      "metadata": {
        "id": "rNtxxfEklfyv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Próximos Passos**"
      ],
      "metadata": {
        "id": "2Va0gb0WlyTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Análise de Resultados: Realizar uma análise detalhada dos resultados, incluindo métricas de avaliação como precisão, recall e F1-Score. A curva ROC e a AUC também podem ser úteis para entender a performance do modelo.\n",
        "\n",
        "Otimização de Hiperparâmetros: Explorar diferentes valores de hiperparâmetros, como taxa de aprendizado, tamanho do batch e número de épocas, para encontrar a configuração que ofereça o melhor desempenho.\n",
        "\n",
        "Integração com Outros Modelos: Experimentar a combinação do BERT com outros modelos baseados em redes neurais convolucionais (CNNs) ou recorrentes (RNNs) para lidar melhor com diferentes tipos de textos."
      ],
      "metadata": {
        "id": "zzJZRwqGl1wn"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}